{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmf/envs/tfgpu/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import rl\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from __future__ import division\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint\n",
    "from keras.layers import Conv1D, Dense, Dropout, BatchNormalization, MaxPooling1D\n",
    "from keras.layers import Embedding, Flatten, Input\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'agent' from 'agent.pyc'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utilities as util\n",
    "import agent as local_agent\n",
    "reload(util)\n",
    "reload(local_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "log_id = util.get_log_id(\"../logs\")\n",
    "os.makedirs(\"../logs/log_{}\".format(log_id))\n",
    "\n",
    "# load in some real and generated documents based on log_ids\n",
    "gen_doc_log_ids = [0, 38, 40, 42, 48, 68]\n",
    "real_recs_train = util.get_real_docs(data_dir=\"../data/real/\")\n",
    "real_recs_val = util.get_real_docs(data_dir=\"../data/real/\", train=False)\n",
    "\n",
    "gen_recs_train = util.get_generated_docs(data_dir=\"../data/generated/\", log_ids=gen_doc_log_ids)\n",
    "gen_recs_val = util.get_generated_docs(data_dir=\"../data/generated/\", log_ids=gen_doc_log_ids, train=False)\n",
    "gen_doc_log_ids.append(log_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluators as ev\n",
    "import text_env as te\n",
    "import processor as proc\n",
    "reload(te)\n",
    "reload(proc)\n",
    "reload(ev)\n",
    "tp = proc.TextProcessor(te.DEFAULT_CHAR_DICT, 50)\n",
    "env = te.Environment([], te.DEFAULT_CHAR_DICT, tp.max_len, log_id)\n",
    "\n",
    "# set the env's real vs. generated documents, both train and val/dev set\n",
    "env.real_docs_train = real_recs_train\n",
    "env.real_docs_val = real_recs_val\n",
    "env.gen_docs_train = gen_recs_train\n",
    "env.gen_docs_val = gen_recs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify whether to load evaluators or train new ones\n",
    "\n",
    "#discrim_path = None\n",
    "discrim_log_id = 74\n",
    "discrim_path = \"../models/discriminators/discrim_{dtype}_{logid}\"\n",
    "kl_path = \"../models/discriminators/{dtype}_{logid}\"\n",
    "\n",
    "#whole_disc = util.Discriminator(\"global_{}\".format(log_id), 600, True, text_proc)\n",
    "local_disc = ev.Discriminator(\"local_{}\".format(log_id), 20, False, env.char_dict)\n",
    "word_disc = ev.Discriminator(\"word_{}\".format(log_id), 8, False, env.char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from ../models/discriminators/discrim_local_74\n",
      "loading from ../models/discriminators/discrim_word_74\n"
     ]
    }
   ],
   "source": [
    "\n",
    "klcalc = ev.KLCalculator(\"topic_model_{}\".format(log_id), True,\n",
    "                         lda_args={'learning_method': 'batch',\n",
    "                                   'n_components': 30, 'n_jobs': 1}\n",
    "                          )\n",
    "\n",
    "if discrim_path is None:\n",
    "#    whole_disc.get_model(text_processor=text_proc)\n",
    "    local_disc.get_model(text_processor=text_proc)\n",
    "    word_disc.get_model(text_processor=text_proc)\n",
    "\n",
    "    local_disc.fit(text_proc, real_recs_train+gen_recs_train, real_recs_val+gen_recs_val,\n",
    "                   np.append(np.ones(len(real_recs_train)),np.zeros(len(gen_recs_train))),\n",
    "                   np.append(np.ones(len(real_recs_val)),np.zeros(len(gen_recs_val))), num_per=5,\n",
    "                   epochs=2)\n",
    "#    whole_disc.fit(text_proc, real_recs_train+gen_recs_train, real_recs_val+gen_recs_val,\n",
    "#                   np.append(np.ones(len(real_recs_train)),np.zeros(len(gen_recs_train))),\n",
    "#                   np.append(np.ones(len(real_recs_val)),np.zeros(len(gen_recs_val))),\n",
    "#                   whole_doc=True,\n",
    "#                   epochs=1)\n",
    "    word_disc.fit(text_proc, real_recs_train+gen_recs_train, real_recs_val+gen_recs_val,\n",
    "                  np.append(np.ones(len(real_recs_train)),np.zeros(len(gen_recs_train))),\n",
    "                  np.append(np.ones(len(real_recs_val)),np.zeros(len(gen_recs_val))), num_per=20,\n",
    "                  epochs=3)\n",
    "    klcalc.fit(env)\n",
    "else:\n",
    "    #whole_disc.get_model(path=discrim_path.format(dtype='global', logid=discrim_log_id))\n",
    "    local_disc.get_model(path=discrim_path.format(dtype='local', logid=discrim_log_id))\n",
    "    cd = local_disc.char_dict\n",
    "    word_disc.get_model(path=discrim_path.format(dtype='word', logid=discrim_log_id))\n",
    "\n",
    "    klcalc.get_model(model_path=kl_path.format(dtype='topic_model', logid=discrim_log_id))\n",
    "\n",
    "\n",
    "env.evaluators.append(local_disc)\n",
    "env.evaluators.append(word_disc)\n",
    "env.evaluators.append(klcalc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LENGTH = 1\n",
    "\n",
    "filter_size = 64\n",
    "embedding_size = 16\n",
    "num_blocks = 2\n",
    "char_inp = Input(shape=(env.max_len,))\n",
    "emb = Embedding(len(env.char_dict), embedding_size)(char_inp)\n",
    "layer_in = emb\n",
    "for n in range(0, num_blocks):\n",
    "    m = 'conv_{}{}'\n",
    "    conv1 = Conv1D(filter_size, 5, padding=\"same\", dilation_rate=1,\n",
    "                   activation='tanh',\n",
    "                   name=m.format(n, 'a'))(layer_in)\n",
    "    conv2 = Conv1D(filter_size, 5, padding=\"same\", dilation_rate=1,\n",
    "                   activation='tanh',\n",
    "                   name=m.format(n, 'b'))(conv1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Dropout(0.25)(conv2)\n",
    "    conv3 = Conv1D(filter_size, 5, padding=\"same\", dilation_rate=2,\n",
    "                   activation='tanh',\n",
    "                   name=m.format(n, 'c'))(conv2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Dropout(0.25)(conv3)\n",
    "    conv4 = Conv1D(filter_size, 5, padding=\"same\", dilation_rate=4,\n",
    "                   activation='tanh',\n",
    "                   name=m.format(n, 'd'))(conv3)\n",
    "    pool = MaxPooling1D(pool_size=5)(conv4)\n",
    "    layer_in = pool\n",
    "flat = Dropout(0.25)(Flatten()(conv4))\n",
    "d = Dense(env.num_actions)(flat)\n",
    "model = Model(char_inp, d)\n",
    "\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.1, value_test=.05,\n",
    "                              nb_steps=1000000)\n",
    "dqn = DQNAgent(model=model, nb_actions=env.num_actions, policy=policy, memory=memory,\n",
    "               processor=tp, nb_steps_warmup=500, gamma=.99, target_model_update=100,\n",
    "               train_interval=4, delta_clip=1.)\n",
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      " 502/1000 [==============>...............] - ETA: 6s - reward: -0.9499"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ed36137a48cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mModelIntervalCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_weights_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mFileLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# After training is done, we save the final weights one more time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jmf/envs/tfgpu/local/lib/python2.7/site-packages/rl/core.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    192\u001b[0m                     \u001b[0;31m# Force a terminal state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m                 \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m                 \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jmf/envs/tfgpu/local/lib/python2.7/site-packages/rl/agents/dqn.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, reward, terminal)\u001b[0m\n\u001b[1;32m    283\u001b[0m                 \u001b[0;31m# while the target network is used to estimate the Q value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m                 \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weights_filename = '../models/dqns/dqn_{}/weights.h5f'.format(log_id)\n",
    "checkpoint_weights_filename = '../models/log_' + str(log_id) + '/dqn_weights_{step}.h5f'\n",
    "log_filename = '../logs/log_{}/dqn_log.json'.format(log_id)\n",
    "callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
    "callbacks += [FileLogger(log_filename, interval=100)]\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=5000, log_interval=1000)\n",
    "\n",
    "# After training is done, we save the final weights one more time.\n",
    "dqn.save_weights(weights_filename, overwrite=True)\n",
    "\n",
    "# Finally, evaluate our algorithm for 10 episodes.\n",
    "dqn.test(env, nb_episodes=10, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 51)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.model.predict_on_batch(np.zeros((32, 50))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = dqn.recent_observation\n",
    "state = dqn.memory.get_recent_state(observation)\n",
    "print(observation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
