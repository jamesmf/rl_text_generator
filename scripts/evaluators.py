import re
import os
import random
import json
import pickle
from collections import namedtuple
import numpy as np
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer
from scipy.stats import entropy
from keras.preprocessing.sequence import pad_sequences
from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint
from keras.layers import Input, Dense, Conv1D, Embedding, Flatten, GlobalMaxPooling1D
from keras.layers import MaxPooling1D, Add, Dropout, BatchNormalization, Concatenate
from keras.optimizers import Adam
from keras.models import Model, load_model


class Discriminator():
    storage_dir = "../models/discriminators/"
    log_dir = "../logs/discriminators/"

    def __init__(self, name, max_len, reward_only_on_end, char_dict,
                 reward_norm=1):
        """
        Discriminators predict whether or not a document was generated by one
        of the previous rounds of trained generators or if it was a part of the
        original input documents. The ability to fool a discriminator is one
        thing we will reward our agents for.
        Arguments:
            name (str): name of the discriminator
            max_len (int): number of characters to consider in a document
            reward_only_on_end (Boolean): whether to reward only at final step
        """
        self.name = name        
        self.max_len = max_len
        self.reward_only_on_end = reward_only_on_end
        self.char_dict = char_dict
        # make the per-action rewards smaller than the whole-document rewards
        if reward_only_on_end:
            self.reward_norm = 1
        else:
            self.reward_norm = reward_norm

    def vectorize(self, txt, start=0, end=None):
        if end is None:
            end = len(txt)
        s = [[self.char_dict[i] for i in txt[start:end]]]
        return pad_sequences(s, maxlen=self.max_len, dtype=np.uint16)[0]

    def evaluate(self, env):
        done = env.episode_done
        state = env.state_text
        if done and self.reward_only_on_end:
            state_vec = self.vectorize(state)
            r = self.model.predict(state_vec.reshape(1, -1))[0][0] - 0.5
#            print("global reward: {}".format(r))
            return r * self.reward_norm
        elif not self.reward_only_on_end:
            state_vec = self.vectorize(state)
            r = self.model.predict(state_vec.reshape(1, -1))[0][0] - 0.5
#            print("local reward: {}".format(r))
            return r * self.reward_norm
        return 0

    def get_model(self, path=None, **kwargs):
        """
        Define or load a model
        """
        if path:
            print("loading from {}".format(path))
            self.model = load_model(path)
            self.max_len = self.model.input_shape[1]
            with open(path+'_map.json', 'r') as f:
                self.char_dict = json.load(f)
            self.char_rev = {v: k for k,v in self.char_dict.items()}
        else:
            filter_size = kwargs.get("filter_size", 256)
            embedding_size = kwargs.get("embedding_size", 16)
            num_blocks = kwargs.get("num_blocks", 1)
            char_inp = Input(shape=(self.max_len,))
            emb = Embedding(len(self.char_dict), embedding_size)(char_inp)
            layer_in = emb
            for n in range(0, num_blocks):
                conv1 = Conv1D(filter_size, 5, padding="same", dilation_rate=1,
                               activation='relu', name='conv_1a')(layer_in)
                conv2 = Conv1D(filter_size, 5, padding="same", dilation_rate=1,
                               activation='relu', name='conv_1b')(conv1)
                conv2 = BatchNormalization()(conv2)
                conv2 = Dropout(0.25)(conv2)
                conv3 = Conv1D(filter_size, 5, padding="same", dilation_rate=2,
                               activation='relu', name='conv_1c')(conv2)
                conv3 = BatchNormalization()(conv3)
                conv3 = Dropout(0.25)(conv3)
                conv4 = Conv1D(filter_size, 5, padding="same", dilation_rate=4,
                               activation='relu', name='conv_1d')(conv3)
                layer_in = conv4
            gmp = GlobalMaxPooling1D()(conv4)
            out = Dense(1, activation='sigmoid', name=self.name+'_out')(gmp)
            model = Model(char_inp, out)
            opt = Adam(0.001)
            model.compile(opt, 'binary_crossentropy')
            self.model = model

    def update(self, env, epochs=1, num_per=1):
        """
        
        """
        whole_doc = self.reward_only_on_end
        gen_train = env.gen_docs_train
        gen_val = env.gen_docs_val
        real_train = env.real_docs_train
        real_val = env.real_docs_val
        self.fit(real_train+gen_train, real_val+gen_val,
                 np.append(np.ones(len(real_train)),
                           np.zeros(len(gen_train))),
                 np.append(np.ones(len(real_val)), 
                           np.zeros(len(gen_val))), num_per=num_per,
                 epochs=epochs, shuffle=True, whole_doc=whole_doc)
        
    def fit(self, train_docs, val_docs, train_labels,
            val_labels, num_per=1, epochs=5, shuffle=False, whole_doc=False):
        """
        Taking in a sample of training documents, some original data and some
        generated (by any historical generator), fit a discriminator that sees
        a sample of max_len characters from the document.
        """
        X_train = np.zeros((len(train_docs)*num_per, self.max_len))
        X_val = np.zeros((len(val_docs)*num_per, self.max_len))
        y_train = np.zeros((len(train_docs)*num_per, 1))
        y_val = np.zeros((len(val_docs)*num_per, 1))

        for n, doc in enumerate(train_docs):
            for i in range(0, num_per):
                n2 = n*num_per + i
                if whole_doc:
                    if len(doc) <= self.max_len:
                        start_ind = 0
                        end_ind = len(doc)
                    else:
                        start_ind = np.random.randint(0, len(doc)-self.max_len)
                        end_ind = start_ind + self.max_len
                else:
                    end_ind = np.random.randint(0, len(doc))
                    start_ind = max(0, end_ind-self.max_len)
                x = self.vectorize(doc, start=start_ind,
                                   end=end_ind)
                X_train[n2, :] = x
                y_train[n2] = train_labels[n]
        for n, doc in enumerate(val_docs):
            for i in range(0, num_per):
                n2 = n*num_per + i
                if whole_doc:
                    if len(doc) <= self.max_len:
                        start_ind = 0
                        end_ind = len(doc)
                    else:
                        start_ind = np.random.randint(0, len(doc)-self.max_len)
                        end_ind = start_ind + self.max_len
                else:
                    end_ind = np.random.randint(0, len(doc))
                    start_ind = max(0, end_ind-self.max_len)
                x = self.vectorize(doc, start=start_ind, end=end_ind)
                X_val[n2, :] = x
                y_val[n2] = val_labels[n]
        callbacks = [
            EarlyStopping(patience=2, monitor='val_loss'),
            ModelCheckpoint(filepath=self.storage_dir+'discrim_'+self.name,
                            verbose=1, save_best_only=True,
                            monitor='val_loss'),
            TensorBoard(log_dir='{}/{}'.format(self.log_dir, self.name))
        ]
        self.model.fit(X_train, y_train, epochs=epochs,
                       validation_data=(X_val, y_val),
                       callbacks=callbacks)
        self.model = load_model(self.storage_dir+'discrim_'+self.name)
        with open(self.storage_dir+'discrim_'+self.name+'_map.json', 'w') as f:
            f.write(json.dumps(self.char_dict))


class KLCalculator:
    storage_dir = "../models/discriminators/"

    def __init__(self, name, reward_only_on_end, cv_args={},
                 lda_args={'n_components': 30}, max_reward=2):
        self.name = name
        self.reward_only_on_end = reward_only_on_end
        self.cv = CountVectorizer(**cv_args)
        self.model = LatentDirichletAllocation(**lda_args)
        self.max_reward = max_reward
        if reward_only_on_end:
            self.reward_norm = 1.0/self.max_reward
        else:
            self.reward_norm = 1.0/600./self.max_reward

    def fit(self, env):
        docs = env.real_docs_train
        x = self.cv.fit_transform(docs)
        self.model.fit(x)
        with open(self.storage_dir+self.name, 'wb') as f:
            pickle.dump(self, f)

    def evaluate(self, env):
        if env.episode_done or (not self.reward_only_on_end):
            odoc = [env.original_doc]
            ndoc = [env.state_text]
            x0 = self.model.transform(self.cv.transform(odoc))[0]+1e-8
            x1 = self.model.transform(self.cv.transform(ndoc))[0]+1e-8
            kldiv = entropy(x0, x1)
            reward = max(self.max_reward - kldiv, 0)
            return reward*self.reward_norm
        return 0.

    def update(self, env):
        pass

    def get_model(self, model_path):
        with open(model_path, 'rb') as f:
            obj = pickle.load(f)
        self.model = obj.model
        self.cv = obj.cv
        